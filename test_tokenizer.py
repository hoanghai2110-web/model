
import torch
from transformers import AutoTokenizer

def test_qwen2_tokenizer():
    """Test Qwen2 tokenizer v·ªõi ti·∫øng Vi·ªát"""
    
    print("üß™ Testing Qwen2 Tokenizer")
    print("="*50)
    
    # Load tokenizer
    try:
        tokenizer = AutoTokenizer.from_pretrained("./qwen2_tokenizer", local_files_only=True)
        print(f"‚úÖ Loaded Qwen2 tokenizer")
        print(f"üìä Vocab size: {tokenizer.vocab_size}")
        print(f"üî§ Special tokens:")
        print(f"   EOS: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
        print(f"   PAD: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
        print(f"   UNK: {tokenizer.unk_token}")
        
        # Setup pad token n·∫øu c·∫ßn
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id
            
    except Exception as e:
        print(f"‚ùå Error loading tokenizer: {e}")
        return
    
    # Test v·ªõi c√°c c√¢u ti·∫øng Vi·ªát
    test_sentences = [
        "Xin ch√†o, b·∫°n kh·ªèe kh√¥ng?",
        "T√¥i y√™u Vi·ªát Nam",
        "H√¥m nay th·ªùi ti·∫øt ƒë·∫πp qu√°!",
        "B·∫°n c√≥ th·ªÉ gi√∫p t√¥i kh√¥ng?",
        "C·∫£m ∆°n b·∫°n r·∫•t nhi·ªÅu"
    ]
    
    print(f"\nüáªüá≥ Testing Vietnamese sentences:")
    print("-" * 50)
    
    for i, sentence in enumerate(test_sentences, 1):
        print(f"\n[{i}] Original: {sentence}")
        
        # Encode
        tokens = tokenizer.encode(sentence, add_special_tokens=True)
        print(f"    Tokens: {tokens[:10]}... (length: {len(tokens)})")
        
        # Decode
        decoded = tokenizer.decode(tokens, skip_special_tokens=True)
        print(f"    Decoded: {decoded}")
        
        # Check quality
        if decoded.strip() == sentence.strip():
            print(f"    ‚úÖ Perfect match!")
        else:
            print(f"    ‚ö†Ô∏è  Slight difference")
    
    # Test conversation format
    print(f"\nüí¨ Testing conversation format:")
    print("-" * 50)
    
    user_msg = "B·∫°n l√† ai?"
    assistant_msg = "T√¥i l√† HyperMamba, tr·ª£ l√Ω AI c·ªßa b·∫°n!"
    
    conversation = f"<|im_start|>user\n{user_msg}<|im_end|>\n<|im_start|>assistant\n{assistant_msg}<|im_end|>"
    
    print(f"Conversation format:")
    print(conversation)
    
    tokens = tokenizer.encode(conversation, add_special_tokens=True)
    decoded = tokenizer.decode(tokens, skip_special_tokens=False)
    
    print(f"\nTokens length: {len(tokens)}")
    print(f"Decoded:\n{decoded}")

if __name__ == "__main__":
    test_qwen2_tokenizer()
