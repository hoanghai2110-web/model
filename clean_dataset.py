
import json
import re
from typing import List, Dict, Any

def is_valid_message(content: str) -> bool:
    """Ki·ªÉm tra xem n·ªôi dung tin nh·∫Øn c√≥ h·ª£p l·ªá kh√¥ng"""
    if not content or not isinstance(content, str):
        return False
    
    # Lo·∫°i b·ªè tin nh·∫Øn qu√° ng·∫Øn (d∆∞·ªõi 3 k√Ω t·ª±)
    if len(content.strip()) < 3:
        return False
    
    # Lo·∫°i b·ªè tin nh·∫Øn ch·ªâ c√≥ k√Ω t·ª± ƒë·∫∑c bi·ªát ho·∫∑c s·ªë
    if not re.search(r'[a-zA-Z√Ä-·ªπ]', content):
        return False
    
    # Lo·∫°i b·ªè tin nh·∫Øn c√≥ qu√° nhi·ªÅu k√Ω t·ª± l·∫∑p
    if re.search(r'(.)\1{5,}', content):
        return False
    
    # Lo·∫°i b·ªè tin nh·∫Øn c√≥ qu√° nhi·ªÅu emoji ho·∫∑c k√Ω t·ª± ƒë·∫∑c bi·ªát
    special_chars = len(re.findall(r'[^\w\s\.,?!√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë]', content))
    if special_chars > len(content) * 0.3:
        return False
    
    return True

def is_valid_conversation(data: Dict[str, Any]) -> bool:
    """Ki·ªÉm tra xem cu·ªôc h·ªôi tho·∫°i c√≥ h·ª£p l·ªá kh√¥ng"""
    if not isinstance(data, dict) or 'messages' not in data:
        return False
    
    messages = data['messages']
    if not isinstance(messages, list) or len(messages) < 2:
        return False
    
    # Ki·ªÉm tra c·∫•u tr√∫c user-assistant
    user_msg = None
    assistant_msg = None
    
    for msg in messages:
        if not isinstance(msg, dict) or 'role' not in msg or 'content' not in msg:
            return False
        
        if msg['role'] == 'user':
            user_msg = msg['content']
        elif msg['role'] == 'assistant':
            assistant_msg = msg['content']
    
    # Ph·∫£i c√≥ c·∫£ user v√† assistant message
    if not user_msg or not assistant_msg:
        return False
    
    # Ki·ªÉm tra n·ªôi dung tin nh·∫Øn
    if not is_valid_message(user_msg) or not is_valid_message(assistant_msg):
        return False
    
    # Lo·∫°i b·ªè cu·ªôc h·ªôi tho·∫°i c√≥ c√¢u tr·∫£ l·ªùi qu√° gi·ªëng nhau
    if user_msg.lower().strip() == assistant_msg.lower().strip():
        return False
    
    return True

def has_inappropriate_content(content: str) -> bool:
    """Ki·ªÉm tra n·ªôi dung kh√¥ng ph√π h·ª£p"""
    inappropriate_keywords = [
        'hack', 'crack', 'pirate', 'torrent', 'cheat',
        'drug', 'kill', 'suicide', 'bomb', 'weapon',
        'sex', 'porn', 'nude', 'nsfw',
        'scam', 'fraud', 'steal', 'rob',
    ]
    
    content_lower = content.lower()
    return any(keyword in content_lower for keyword in inappropriate_keywords)

def clean_dataset(input_file: str = "dataset.jsonl", output_file: str = "dataset_cleaned.jsonl"):
    """L√†m s·∫°ch dataset v√† t·∫°o file m·ªõi"""
    
    print("üßπ B·∫Øt ƒë·∫ßu l√†m s·∫°ch dataset...")
    
    valid_conversations = []
    total_lines = 0
    errors = []
    
    # ƒê·ªçc v√† x·ª≠ l√Ω t·ª´ng d√≤ng
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                total_lines += 1
                line = line.strip()
                
                if not line:
                    continue
                
                try:
                    # Parse JSON
                    data = json.loads(line)
                    
                    # Ki·ªÉm tra t√≠nh h·ª£p l·ªá
                    if is_valid_conversation(data):
                        # Ki·ªÉm tra n·ªôi dung kh√¥ng ph√π h·ª£p
                        user_content = ""
                        assistant_content = ""
                        
                        for msg in data['messages']:
                            if msg['role'] == 'user':
                                user_content = msg['content']
                            elif msg['role'] == 'assistant':
                                assistant_content = msg['content']
                        
                        if (not has_inappropriate_content(user_content) and 
                            not has_inappropriate_content(assistant_content)):
                            valid_conversations.append(data)
                        else:
                            errors.append(f"D√≤ng {line_num}: N·ªôi dung kh√¥ng ph√π h·ª£p")
                    else:
                        errors.append(f"D√≤ng {line_num}: C·∫•u tr√∫c h·ªôi tho·∫°i kh√¥ng h·ª£p l·ªá")
                        
                except json.JSONDecodeError as e:
                    errors.append(f"D√≤ng {line_num}: L·ªói JSON - {str(e)}")
                except Exception as e:
                    errors.append(f"D√≤ng {line_num}: L·ªói kh√°c - {str(e)}")
    
    except FileNotFoundError:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file {input_file}")
        return
    
    # Ghi file m·ªõi
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            for conversation in valid_conversations:
                f.write(json.dumps(conversation, ensure_ascii=False) + '\n')
        
        print(f"‚úÖ ƒê√£ l√†m s·∫°ch dataset:")
        print(f"   üìä T·ªïng s·ªë d√≤ng: {total_lines}")
        print(f"   ‚úÖ D√≤ng h·ª£p l·ªá: {len(valid_conversations)}")
        print(f"   ‚ùå D√≤ng l·ªói: {len(errors)}")
        print(f"   üìà T·ª∑ l·ªá th√†nh c√¥ng: {len(valid_conversations)/total_lines*100:.1f}%")
        print(f"   üíæ File s·∫°ch: {output_file}")
        
        # Hi·ªÉn th·ªã m·ªôt s·ªë l·ªói
        if errors:
            print(f"\n‚ö†Ô∏è  M·ªôt s·ªë l·ªói ph√°t hi·ªán:")
            for error in errors[:10]:  # Ch·ªâ hi·ªÉn th·ªã 10 l·ªói ƒë·∫ßu
                print(f"   {error}")
            if len(errors) > 10:
                print(f"   ... v√† {len(errors) - 10} l·ªói kh√°c")
                
    except Exception as e:
        print(f"‚ùå L·ªói khi ghi file: {e}")

def analyze_dataset(file_path: str = "dataset.jsonl"):
    """Ph√¢n t√≠ch dataset ƒë·ªÉ t√¨m v·∫•n ƒë·ªÅ"""
    
    print("üîç Ph√¢n t√≠ch dataset...")
    
    issues = {
        'json_errors': [],
        'missing_fields': [],
        'short_messages': [],
        'special_chars': [],
        'duplicates': [],
        'inappropriate': []
    }
    
    seen_conversations = set()
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                
                try:
                    data = json.loads(line)
                    
                    # Ki·ªÉm tra c·∫•u tr√∫c
                    if 'messages' not in data:
                        issues['missing_fields'].append(line_num)
                        continue
                    
                    user_content = ""
                    assistant_content = ""
                    
                    for msg in data['messages']:
                        if not isinstance(msg, dict) or 'role' not in msg or 'content' not in msg:
                            issues['missing_fields'].append(line_num)
                            continue
                        
                        if msg['role'] == 'user':
                            user_content = msg['content']
                        elif msg['role'] == 'assistant':
                            assistant_content = msg['content']
                    
                    # Ki·ªÉm tra tin nh·∫Øn ng·∫Øn
                    if len(user_content.strip()) < 3 or len(assistant_content.strip()) < 3:
                        issues['short_messages'].append(line_num)
                    
                    # Ki·ªÉm tra k√Ω t·ª± ƒë·∫∑c bi·ªát
                    if (not re.search(r'[a-zA-Z√Ä-·ªπ]', user_content) or 
                        not re.search(r'[a-zA-Z√Ä-·ªπ]', assistant_content)):
                        issues['special_chars'].append(line_num)
                    
                    # Ki·ªÉm tra tr√πng l·∫∑p
                    conversation_key = f"{user_content.strip()[:50]}|{assistant_content.strip()[:50]}"
                    if conversation_key in seen_conversations:
                        issues['duplicates'].append(line_num)
                    else:
                        seen_conversations.add(conversation_key)
                    
                    # Ki·ªÉm tra n·ªôi dung kh√¥ng ph√π h·ª£p
                    if (has_inappropriate_content(user_content) or 
                        has_inappropriate_content(assistant_content)):
                        issues['inappropriate'].append(line_num)
                        
                except json.JSONDecodeError:
                    issues['json_errors'].append(line_num)
                except Exception:
                    issues['json_errors'].append(line_num)
    
    except FileNotFoundError:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file {file_path}")
        return
    
    # B√°o c√°o k·∫øt qu·∫£
    print(f"\nüìã B√ÅO C√ÅO PH√ÇN T√çCH:")
    total_issues = sum(len(issue_list) for issue_list in issues.values())
    
    if total_issues == 0:
        print("‚úÖ Dataset kh√¥ng c√≥ v·∫•n ƒë·ªÅ g√¨!")
    else:
        for issue_type, line_numbers in issues.items():
            if line_numbers:
                print(f"   {issue_type}: {len(line_numbers)} d√≤ng")
                if len(line_numbers) <= 5:
                    print(f"      D√≤ng: {line_numbers}")
                else:
                    print(f"      D√≤ng: {line_numbers[:5]}... (+{len(line_numbers)-5} d√≤ng kh√°c)")

if __name__ == "__main__":
    print("üõ†Ô∏è  TOOL L·ªåC DATASET")
    print("=" * 40)
    
    # Ph√¢n t√≠ch tr∆∞·ªõc
    analyze_dataset("dataset.jsonl")
    
    print("\n" + "=" * 40)
    
    # L√†m s·∫°ch dataset
    clean_dataset("dataset.jsonl", "dataset_cleaned.jsonl")
    
    print("\n‚ú® Ho√†n th√†nh! S·ª≠ d·ª•ng file 'dataset_cleaned.jsonl' ƒë·ªÉ training.")
