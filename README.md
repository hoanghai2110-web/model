
# HyperMamba - Trá»£ lÃ½ AI tiáº¿ng Viá»‡t

HyperMamba lÃ  má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ nhá» gá»n (130M parameters) Ä‘Æ°á»£c tá»‘i Æ°u cho viá»‡c trÃ² chuyá»‡n báº±ng tiáº¿ng Viá»‡t. MÃ´ hÃ¬nh sá»­ dá»¥ng kiáº¿n trÃºc Transformer Lite vá»›i LoRA (Low-Rank Adaptation) Ä‘á»ƒ fine-tuning hiá»‡u quáº£.

## ğŸš€ TÃ­nh nÄƒng chÃ­nh

- **Nháº¹ vÃ  nhanh**: Chá»‰ 130M parameters, phÃ¹ há»£p cháº¡y trÃªn mÃ¡y tÃ­nh cÃ¡ nhÃ¢n
- **Tá»‘i Æ°u tiáº¿ng Viá»‡t**: Sá»­ dá»¥ng Qwen2 tokenizer vá»›i há»— trá»£ tiáº¿ng Viá»‡t tá»‘t
- **LoRA fine-tuning**: Fine-tune hiá»‡u quáº£ vá»›i Ã­t dá»¯ liá»‡u
- **Kiáº¿n trÃºc hiá»‡n Ä‘áº¡i**: RMSNorm, RoPE, SwiGLU, Gated Attention

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
â”œâ”€â”€ model.py          # Äá»‹nh nghÄ©a mÃ´ hÃ¬nh TransformerLite
â”œâ”€â”€ fine_tune.py      # Script fine-tuning vá»›i LoRA
â”œâ”€â”€ test_model.py     # Script test mÃ´ hÃ¬nh sau fine-tuning
â”œâ”€â”€ test_tokenizer.py # Test Qwen2 tokenizer
â”œâ”€â”€ dataset.jsonl     # Dá»¯ liá»‡u máº«u cho fine-tuning
â”œâ”€â”€ qwen2_tokenizer/  # Qwen2 tokenizer Ä‘Ã£ táº£i vá»
â””â”€â”€ README.md         # File hÆ°á»›ng dáº«n nÃ y
```

## ğŸ› ï¸ CÃ i Ä‘áº·t vÃ  cháº¡y

### 0. CÃ i Ä‘áº·t thÆ° viá»‡n cáº§n thiáº¿t

TrÆ°á»›c tiÃªn, cÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n Python cáº§n thiáº¿t:

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
pip install transformers tokenizers tqdm
```

Hoáº·c náº¿u cÃ³ GPU:
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install transformers tokenizers tqdm
```

**Trong Replit**: CÃ¡c thÆ° viá»‡n sáº½ Ä‘Æ°á»£c tá»± Ä‘á»™ng cÃ i Ä‘áº·t khi cháº¡y code láº§n Ä‘áº§u.

### 1. Fine-tuning mÃ´ hÃ¬nh

TrÆ°á»›c tiÃªn, báº¡n cáº§n fine-tune mÃ´ hÃ¬nh vá»›i dá»¯ liá»‡u tiáº¿ng Viá»‡t:

```bash
python fine_tune.py
```

Hoáº·c sá»­ dá»¥ng workflow trong Replit:
- Chá»n "Fine-tune HyperMamba" tá»« dropdown workflow

**QuÃ¡ trÃ¬nh fine-tuning sáº½:**
- Load mÃ´ hÃ¬nh base vá»›i LoRA configuration
- Äá»c dá»¯ liá»‡u tá»« `dataset.jsonl`
- Fine-tune trong 3 epochs
- LÆ°u LoRA weights vÃ o folder `hypermamba_finetuned/`

### 2. Test mÃ´ hÃ¬nh

Sau khi fine-tuning xong, test mÃ´ hÃ¬nh:

```bash
python test_model.py
```

Hoáº·c sá»­ dá»¥ng workflow:
- Chá»n "Test HyperMamba" tá»« dropdown workflow

**Cháº¿ Ä‘á»™ test:**
1. **Interactive chat**: TrÃ² chuyá»‡n tÆ°Æ¡ng tÃ¡c vá»›i mÃ´ hÃ¬nh
2. **Predefined questions**: Test vá»›i cÃ¢u há»i cÃ³ sáºµn

## ğŸ“Š Dá»¯ liá»‡u training

File `dataset.jsonl` chá»©a dá»¯ liá»‡u máº«u theo format:

```json
{
  "messages": [
    {"role": "user", "content": "Danh tÃ­nh cá»§a báº¡n?"},
    {"role": "assistant", "content": "Danh tÃ­nh cá»§a mÃ¬nh lÃ  HyperMamba..."}
  ]
}
```

### ThÃªm dá»¯ liá»‡u má»›i:
1. Má»Ÿ file `dataset.jsonl`
2. ThÃªm dÃ²ng má»›i theo format trÃªn
3. Cháº¡y láº¡i fine-tuning

## âš™ï¸ Cáº¥u hÃ¬nh mÃ´ hÃ¬nh

### ThÃ´ng sá»‘ mÃ´ hÃ¬nh chÃ­nh:
- **Vocabulary size**: 151,657 (Qwen2 tokenizer)
- **Hidden size**: 768
- **Layers**: 12
- **Attention heads**: 12
- **Context length**: 4,096 tokens

### ThÃ´ng sá»‘ LoRA:
- **Rank**: 16
- **Alpha**: 32  
- **Dropout**: 0.1
- **Target modules**: qkv_proj, out_proj, gate_proj, up_proj, down_proj

## ğŸ¯ Sá»­ dá»¥ng

### 1. Cháº¡y interactive chat:

```bash
python test_model.py
# Chá»n option 1 khi Ä‘Æ°á»£c há»i
```

VÃ­ dá»¥ conversation:
```
ğŸ‘¤ Báº¡n: Xin chÃ o!
ğŸ¤– HyperMamba: Xin chÃ o! MÃ¬nh lÃ  HyperMamba, ráº¥t vui Ä‘Æ°á»£c gáº·p báº¡n!

ğŸ‘¤ Báº¡n: Báº¡n cÃ³ thá»ƒ lÃ m gÃ¬?
ğŸ¤– HyperMamba: MÃ¬nh cÃ³ thá»ƒ trÃ² chuyá»‡n vá»›i báº¡n, tráº£ lá»i cÃ¢u há»i, ká»ƒ chuyá»‡n...
```

### 2. Test vá»›i cÃ¢u há»i cÃ³ sáºµn:

```bash
python test_model.py  
# Chá»n option 2 khi Ä‘Æ°á»£c há»i
```

## ğŸ”§ Tuning vÃ  tá»‘i Æ°u

### Äá»ƒ cáº£i thiá»‡n cháº¥t lÆ°á»£ng:

1. **ThÃªm dá»¯ liá»‡u**: Bá»• sung cÃ¢u há»i-tráº£ lá»i vÃ o `dataset.jsonl`
2. **TÄƒng epochs**: Sá»­a `num_epochs` trong `fine_tune.py`
3. **Äiá»u chá»‰nh learning rate**: Thay Ä‘á»•i `lr` trong optimizer
4. **TÄƒng LoRA rank**: Sá»­a `lora_rank` trong config (tá»‘n nhiá»u memory hÆ¡n)

### Monitoring training:

Fine-tuning sáº½ hiá»ƒn thá»‹:
- Loss theo tá»«ng step
- Average loss theo epoch
- Learning rate hiá»‡n táº¡i
- ThÃ´ng tin lÆ°u model

## ğŸ› Troubleshooting

### Lá»—i thÆ°á»ng gáº·p:

1. **Import errors (torch, transformers)**:
   ```bash
   pip install torch transformers tokenizers tqdm
   ```
   Hoáº·c trong Replit: Chá» tá»± Ä‘á»™ng cÃ i Ä‘áº·t khi cháº¡y code

2. **CUDA out of memory**: 
   - Giáº£m `batch_size` trong `fine_tune.py`
   - Giáº£m `max_length` trong dataset

3. **Tokenizer khÃ´ng load Ä‘Æ°á»£c**:
   - Kiá»ƒm tra folder `mistral_tokenizer/` cÃ³ Ä‘áº§y Ä‘á»§ files
   - Model sáº½ tá»± Ä‘á»™ng fallback vá» remote tokenizer

4. **Response khÃ´ng tá»‘t**:
   - ThÃªm nhiá»u dá»¯ liá»‡u training
   - TÄƒng sá»‘ epochs
   - Kiá»ƒm tra format dá»¯ liá»‡u trong `dataset.jsonl`

5. **ModuleNotFoundError**:
   - Äáº£m báº£o Ä‘Ã£ cÃ i Ä‘áº·t Ä‘á»§ thÆ° viá»‡n
   - Restart kernel náº¿u cáº§n

## ğŸ“ˆ Performance

### ThÃ´ng sá»‘ hiá»‡u suáº¥t:
- **Total parameters**: ~130M
- **Trainable LoRA parameters**: ~2M (1.5% cá»§a total)
- **Memory usage**: ~2-4GB VRAM (tÃ¹y batch size)
- **Training time**: ~5-10 phÃºt (3 epochs trÃªn CPU/GPU)

## ğŸ¤ ÄÃ³ng gÃ³p

Äá»ƒ cáº£i thiá»‡n mÃ´ hÃ¬nh:
1. ThÃªm dá»¯ liá»‡u conversation cháº¥t lÆ°á»£ng cao
2. Tá»‘i Æ°u hyperparameters
3. Thá»­ nghiá»‡m kiáº¿n trÃºc má»›i

## ğŸ“ LÆ°u Ã½

- MÃ´ hÃ¬nh Ä‘Æ°á»£c thiáº¿t káº¿ cho má»¥c Ä‘Ã­ch há»c táº­p vÃ  thá»­ nghiá»‡m
- Cháº¥t lÆ°á»£ng phá»¥ thuá»™c vÃ o dá»¯ liá»‡u training
- Khuyáº¿n nghá»‹ fine-tune vá»›i dataset lá»›n hÆ¡n cho production

---

**Happy coding! ğŸš€**

Náº¿u cÃ³ váº¥n Ä‘á» gÃ¬, hÃ£y kiá»ƒm tra console output Ä‘á»ƒ debug.
